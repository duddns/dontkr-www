<!DOCTYPE html>
<html lang="ko" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>머신러닝 개념 정리 | DoNT - Do Not Think!!!</title>
<meta name="keywords" content="ai, machine learning, deep learning">
<meta name="description" content="1. 머신러닝 전체 프로세스

  
      
          단계
          전통 머신러닝
          딥러닝
      
  
  
      
          1. 원본 데이터
          텍스트, 이미지, 표 등
          텍스트, 이미지, 표 등
      
      
          2. 분할
          텍스트→토큰, 이미지→픽셀, 음성→프레임
          텍스트→토큰, 이미지→픽셀, 음성→프레임
      
      
          3. 벡터화
          숫자로 변환 (ID, RGB, 정규화)
          숫자로 변환 (ID, RGB, 정규화)
      
      
          4. Feature 추출
          사람이 설계 (고정)- 긍정단어 개수- 부정단어 개수- 느낌표 개수
          -
      
      
          5. 학습 초기화
          랜덤 Parameter
          랜덤 Parameter
      
      
          6. 순전파
          Feature × Parameter = 예측
          벡터화된 입력 × Parameter→ Layer별 Feature 자동 생성→ 예측
      
      
          7. 손실 계산
          정답과 비교
          정답과 비교
      
      
          8. 역전파
          Parameter 조정
          Parameter 조정→ Feature 표현도 변화
      
      
          9. 반복
          68 반복 (수백만수억 회)
          68 반복 (수백만수억 회)
      
      
          10. 학습된 모델
          고정 Feature &#43; 학습된 Parameter
          학습된 Parameter(Feature 표현 내장)
      
      
          11. 추론: 전처리
          분할 &#43; 벡터화
          분할 &#43; 벡터화
      
      
          12. 추론: Feature
          같은 방식으로 Feature 추출
          학습된 모델로 자동 생성
      
      
          13. 추론: 예측
          학습된 Parameter로 계산
          학습된 Parameter로 계산
      
      
          14. 예측 결과
          출력
          출력
      
  


2. 핵심 용어 완전 정리
데이터 (Data)
원본 정보">
<meta name="author" content="">
<link rel="canonical" href="https://dont.kr/posts/2025/2025-11-12-ml-fundamentals/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f56f6fcbbbf1b56135b161970b2be054c0f93cd0507af71fddbab4cf707c5c56.css" integrity="sha256-9W9vy7vxtWE1sWGXCyvgVMD5PNBQevcf3bq0z3B8XFY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://dont.kr/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://dont.kr/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://dont.kr/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://dont.kr/apple-touch-icon.png">
<link rel="mask-icon" href="https://dont.kr/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="ko" href="https://dont.kr/posts/2025/2025-11-12-ml-fundamentals/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZVEM176XM1"></script>
      <script>
        var doNotTrack = false;
        if ( true ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-ZVEM176XM1');
        }
      </script><meta property="og:url" content="https://dont.kr/posts/2025/2025-11-12-ml-fundamentals/">
  <meta property="og:site_name" content="DoNT - Do Not Think!!!">
  <meta property="og:title" content="머신러닝 개념 정리">
  <meta property="og:description" content="1. 머신러닝 전체 프로세스 단계 전통 머신러닝 딥러닝 1. 원본 데이터 텍스트, 이미지, 표 등 텍스트, 이미지, 표 등 2. 분할 텍스트→토큰, 이미지→픽셀, 음성→프레임 텍스트→토큰, 이미지→픽셀, 음성→프레임 3. 벡터화 숫자로 변환 (ID, RGB, 정규화) 숫자로 변환 (ID, RGB, 정규화) 4. Feature 추출 사람이 설계 (고정)- 긍정단어 개수- 부정단어 개수- 느낌표 개수 - 5. 학습 초기화 랜덤 Parameter 랜덤 Parameter 6. 순전파 Feature × Parameter = 예측 벡터화된 입력 × Parameter→ Layer별 Feature 자동 생성→ 예측 7. 손실 계산 정답과 비교 정답과 비교 8. 역전파 Parameter 조정 Parameter 조정→ Feature 표현도 변화 9. 반복 68 반복 (수백만수억 회) 68 반복 (수백만수억 회) 10. 학습된 모델 고정 Feature &#43; 학습된 Parameter 학습된 Parameter(Feature 표현 내장) 11. 추론: 전처리 분할 &#43; 벡터화 분할 &#43; 벡터화 12. 추론: Feature 같은 방식으로 Feature 추출 학습된 모델로 자동 생성 13. 추론: 예측 학습된 Parameter로 계산 학습된 Parameter로 계산 14. 예측 결과 출력 출력 2. 핵심 용어 완전 정리 데이터 (Data) 원본 정보">
  <meta property="og:locale" content="ko-kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-12T00:00:00+09:00">
    <meta property="article:modified_time" content="2025-11-12T00:00:00+09:00">
    <meta property="article:tag" content="Ai">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="머신러닝 개념 정리">
<meta name="twitter:description" content="1. 머신러닝 전체 프로세스

  
      
          단계
          전통 머신러닝
          딥러닝
      
  
  
      
          1. 원본 데이터
          텍스트, 이미지, 표 등
          텍스트, 이미지, 표 등
      
      
          2. 분할
          텍스트→토큰, 이미지→픽셀, 음성→프레임
          텍스트→토큰, 이미지→픽셀, 음성→프레임
      
      
          3. 벡터화
          숫자로 변환 (ID, RGB, 정규화)
          숫자로 변환 (ID, RGB, 정규화)
      
      
          4. Feature 추출
          사람이 설계 (고정)- 긍정단어 개수- 부정단어 개수- 느낌표 개수
          -
      
      
          5. 학습 초기화
          랜덤 Parameter
          랜덤 Parameter
      
      
          6. 순전파
          Feature × Parameter = 예측
          벡터화된 입력 × Parameter→ Layer별 Feature 자동 생성→ 예측
      
      
          7. 손실 계산
          정답과 비교
          정답과 비교
      
      
          8. 역전파
          Parameter 조정
          Parameter 조정→ Feature 표현도 변화
      
      
          9. 반복
          68 반복 (수백만수억 회)
          68 반복 (수백만수억 회)
      
      
          10. 학습된 모델
          고정 Feature &#43; 학습된 Parameter
          학습된 Parameter(Feature 표현 내장)
      
      
          11. 추론: 전처리
          분할 &#43; 벡터화
          분할 &#43; 벡터화
      
      
          12. 추론: Feature
          같은 방식으로 Feature 추출
          학습된 모델로 자동 생성
      
      
          13. 추론: 예측
          학습된 Parameter로 계산
          학습된 Parameter로 계산
      
      
          14. 예측 결과
          출력
          출력
      
  


2. 핵심 용어 완전 정리
데이터 (Data)
원본 정보">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://dont.kr/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "머신러닝 개념 정리",
      "item": "https://dont.kr/posts/2025/2025-11-12-ml-fundamentals/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "머신러닝 개념 정리",
  "name": "머신러닝 개념 정리",
  "description": "1. 머신러닝 전체 프로세스 단계 전통 머신러닝 딥러닝 1. 원본 데이터 텍스트, 이미지, 표 등 텍스트, 이미지, 표 등 2. 분할 텍스트→토큰, 이미지→픽셀, 음성→프레임 텍스트→토큰, 이미지→픽셀, 음성→프레임 3. 벡터화 숫자로 변환 (ID, RGB, 정규화) 숫자로 변환 (ID, RGB, 정규화) 4. Feature 추출 사람이 설계 (고정)- 긍정단어 개수- 부정단어 개수- 느낌표 개수 - 5. 학습 초기화 랜덤 Parameter 랜덤 Parameter 6. 순전파 Feature × Parameter = 예측 벡터화된 입력 × Parameter→ Layer별 Feature 자동 생성→ 예측 7. 손실 계산 정답과 비교 정답과 비교 8. 역전파 Parameter 조정 Parameter 조정→ Feature 표현도 변화 9. 반복 68 반복 (수백만수억 회) 68 반복 (수백만수억 회) 10. 학습된 모델 고정 Feature + 학습된 Parameter 학습된 Parameter(Feature 표현 내장) 11. 추론: 전처리 분할 + 벡터화 분할 + 벡터화 12. 추론: Feature 같은 방식으로 Feature 추출 학습된 모델로 자동 생성 13. 추론: 예측 학습된 Parameter로 계산 학습된 Parameter로 계산 14. 예측 결과 출력 출력 2. 핵심 용어 완전 정리 데이터 (Data) 원본 정보\n",
  "keywords": [
    "ai", "machine learning", "deep learning"
  ],
  "articleBody": "1. 머신러닝 전체 프로세스 단계 전통 머신러닝 딥러닝 1. 원본 데이터 텍스트, 이미지, 표 등 텍스트, 이미지, 표 등 2. 분할 텍스트→토큰, 이미지→픽셀, 음성→프레임 텍스트→토큰, 이미지→픽셀, 음성→프레임 3. 벡터화 숫자로 변환 (ID, RGB, 정규화) 숫자로 변환 (ID, RGB, 정규화) 4. Feature 추출 사람이 설계 (고정)- 긍정단어 개수- 부정단어 개수- 느낌표 개수 - 5. 학습 초기화 랜덤 Parameter 랜덤 Parameter 6. 순전파 Feature × Parameter = 예측 벡터화된 입력 × Parameter→ Layer별 Feature 자동 생성→ 예측 7. 손실 계산 정답과 비교 정답과 비교 8. 역전파 Parameter 조정 Parameter 조정→ Feature 표현도 변화 9. 반복 68 반복 (수백만수억 회) 68 반복 (수백만수억 회) 10. 학습된 모델 고정 Feature + 학습된 Parameter 학습된 Parameter(Feature 표현 내장) 11. 추론: 전처리 분할 + 벡터화 분할 + 벡터화 12. 추론: Feature 같은 방식으로 Feature 추출 학습된 모델로 자동 생성 13. 추론: 예측 학습된 Parameter로 계산 학습된 Parameter로 계산 14. 예측 결과 출력 출력 2. 핵심 용어 완전 정리 데이터 (Data) 원본 정보\n예시 텍스트: “안녕하세요” 이미지: 고양이 사진 표: 집 정보 (평수, 방개수) 역할 학습의 재료\n분할 (Segmentation) 데이터를 처리 가능한 작은 단위로 나누는 과정\n데이터별 분할 텍스트 → 토큰 (Tokenization) 이미지 → 픽셀 (Pixelation) 음성 → 프레임 (Framing) 목적 벡터화하기 전 처리 가능한 단위로 분해\n토큰 (Token) 텍스트를 처리 가능한 작은 단위로 나눈 것\n\"안녕하세요\" → [\"안녕\", \"하세요\"] (2토큰) \"I love you\" → [\"I\", \"love\", \"you\"] (3토큰) 특징 NLP(자연어 처리) 전용 용어 이미지는 “픽셀”, 음성은 “프레임” LLM에서 비용/길이 계산 단위 범위 자연어 처리 전반 (LLM 전용 아님)\n벡터화 (Vectorization) 데이터를 숫자로 변환하는 과정 (표현 방식의 변환)\n\"안녕\" → 1523 (숫자 ID) 빨간 픽셀 → [255, 0, 0] (RGB 값) 목적 기계가 읽을 수 있는 형태로 변환\nFeature 추출 벡터화된 데이터 중 의미있는 부분을 선택하거나 새로 만드는 과정\n벡터화: [1523, 8842, ...] ↓ Feature 추출: 의미있는 표현 생성 ↓ Feature: [3, 0, 2] (긍정단어 개수, 부정단어 개수, 느낌표) 핵심 벡터화는 표현 방식의 변환 Feature 추출은 의미 선택의 과정 둘은 구분되지만 일부 겹침 Feature (특징) 모델이 학습에 활용하는 의미있는 숫자 표현\n전통 머신러닝 예시 (감정 분석) 리뷰 텍스트 → Feature (사람이 설계) - 긍정 단어 개수: 3 - 부정 단어 개수: 0 - 느낌표 개수: 2 → Feature: [3, 0, 2] 딥러닝 예시 (감정 분석) 리뷰 텍스트 → Feature (자동 학습) - 입력: \"이 영화 정말 최고예요!\" - Layer 1: 단어 조합 패턴 - Layer 2: 감정 강도 패턴 - Layer 3: 전체 의도 (극찬) 학습 대상 여부 전통 머신러닝: Feature 고정, Parameter만 학습 딥러닝: Parameter 학습을 통해 Feature 표현도 간접 학습 핵심 모델에 “무엇을 볼 것인가” 제공 딥러닝: 각 Layer 출력도 Feature 전통 머신러닝 vs 딥러닝 전통 머신러닝: 사람이 Feature 설계 (고정) 딥러닝: Feature 자동 학습 (Parameter로 제어) Parameter (파라미터) 모델이 학습으로 조정하는 내부 숫자 값\n예시 감정점수 = (긍정단어 × a) + (부정단어 × b) + (느낌표 × c) a = 0.5 ← 긍정단어: 양수 (점수 올림) b = -0.8 ← 부정단어: 음수 (점수 낮춤) c = 0.3 ← 느낌표: 약한 양수 (약간 긍정) 구성 가중치 (Weight) 편향 (Bias) 핵심 학습으로 조정됨 (최적화 대상) Feature를 “어떻게 처리할지” 결정 크기 구분 Parameter 규모 예시 전통 머신러닝 10² ~ 10⁴ 선형 회귀, SVM CNN (딥러닝) 10⁵ ~ 10⁷ 이미지넷 모델 Transformer 10⁸ ~ 10¹⁰ BERT, GPT-2 LLM 10¹¹ ~ 10¹³ GPT-3, GPT-4 Feature vs Parameter 한눈에 Feature Parameter 정의 의미있는 숫자 표현 학습 가능한 내부 값 생성 방식 데이터에서 계산 랜덤 초기화 후 학습으로 최적화 역할 무엇을 볼 것인가 어떻게 계산할 것인가 예시 [3, 0, 2] (긍정, 부정, 느낌표) a=0.5, b=-0.8, c=0.3 위치 입력/중간 표현 모델 내부 학습 대상 전통 머신러닝: ✗딥러닝: 간접 ✓ ✓ (직접 최적화) 비유 요리 재료 요리 레시피 관계 Feature (입력) × Parameter (가중치) = 출력 (예측) 학습: 출력 → 손실 계산 → Parameter 조정 → 반복 추론: 출력 = 최종 예측 학습 (Training) 데이터로 최적의 Parameter를 찾는 과정\n랜덤 Parameter로 시작 Feature × Parameter = 예측 오차 계산 (틀린 정도) 역전파로 Parameter 조정 반복 (점점 정확해짐) 결과 학습된 모델 (구조 + 최적화된 Parameter)\n딥러닝에서 Parameter 학습으로 Feature 표현도 함께 최적화됨\n모델 (Model) 구조(Architecture) + 학습된 Parameter\n모델 파일 (예: PyTorch .pt, TensorFlow .pb): - 네트워크 구조 정의 - 학습된 Parameter 값들 예시 GPT-3 모델 = Transformer 구조 + 1,750억 개 Parameter = 700GB 추론 (Inference) 학습된 모델로 예측하는 과정\n입력 Feature × 학습된 Parameter = 예측 결과 예시 새 리뷰: “음식 맛있어요!” → Feature 추출 → 모델로 계산 → “긍정 92%” 새 리뷰: “별로예요…” → Feature 추출 → 모델로 계산 → “부정 85%” 사용 새 입력 → 모델 → 예측\n3. 전통 머신러닝 vs 딥러닝 비교 전통 머신러닝 Feature: 사람 설계 (고정) 학습: Parameter만 예시: 선형 회귀, SVM, 랜덤포레스트 딥러닝 Feature: 학습 중 자동 생성 학습: Parameter + Feature 동시 예시: CNN, RNN, Transformer, LLM 비교표 전통 머신러닝 딥러닝 Feature 사람이 설계 (고정) Parameter 학습으로 자동 학습 Parameter 자동 학습 자동 학습 파라미터 수 10² ~ 10⁴ 10⁵ ~ 10¹³ 계산량 적음 많음 (GPU 필요) 데이터 필요량 수천~수만 수백만~수조 학습 시간 분~시간 일~월 복잡도 단순 패턴 복잡한 패턴 전처리 필요 필요 (자동≠불필요) 해석 가능성 높음 상대적으로 낮음 (XAI로 개선 중) 4. 전체 요약 머신러닝 = 데이터로 모델 만들기 데이터 → Feature → Parameter 학습 → 모델 → 예측 전통 ML vs 딥러닝 전통 ML: Feature 사람 설계 딥러닝: Feature 자동 학습 발전 과정 2000년대: 전통 머신러닝 사람이 Feature 설계 Parameter 10² ~ 10⁴ 개 간단한 작업 2010년대: 딥러닝 Feature 자동 학습 Parameter 10⁵ ~ 10⁷ 개 이미지, 음성 인식 성공 GPU 필수 2020년대: LLM 초대규모 Parameter (10¹¹ ~ 10¹³) 언어 이해/생성 Pre-training + Fine-tuning 범용 AI의 시작 5. 실무 관점 정리 AI 도입 시 이해해야 할 것 전통 머신러닝을 선택하는 경우 정형 데이터 (표 형태) 데이터: 수천~수만 건 해석 가능성 중요 빠른 개발 필요 예: 집값 예측, 고객 이탈 예측 딥러닝을 선택하는 경우 비정형 데이터 (이미지, 음성, 텍스트) 데이터: 수십만 건 이상 복잡한 패턴 GPU 인프라 있음 예: 얼굴 인식, 음성 인식 LLM API 사용하는 경우 텍스트 처리 빠른 프로토타입 자체 모델 학습 불필요 예: 챗봇, 요약, 번역 준비 사항 체크리스트 데이터 충분한가? (양, 품질) 리소스 있는가? (GPU, 시간, 비용) Feature 설계 가능한가? (전통 ML) 해석 가능성 필요한가? 실시간 응답 필요한가? ",
  "wordCount" : "989",
  "inLanguage": "ko",
  "datePublished": "2025-11-12T00:00:00+09:00",
  "dateModified": "2025-11-12T00:00:00+09:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dont.kr/posts/2025/2025-11-12-ml-fundamentals/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "DoNT - Do Not Think!!!",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dont.kr/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://dont.kr/" accesskey="h" title="DoNT - Do Not Think!!! (Alt + H)">DoNT - Do Not Think!!!</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://dont.kr/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://dont.kr/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      머신러닝 개념 정리
    </h1>
    <div class="post-meta"><span title='2025-11-12 00:00:00 +0900 KST'>2025년 11월 12일</span>

</div>
  </header> 
  <div class="post-content"><h2 id="1-머신러닝-전체-프로세스">1. 머신러닝 전체 프로세스<a hidden class="anchor" aria-hidden="true" href="#1-머신러닝-전체-프로세스">#</a></h2>
<table>
  <thead>
      <tr>
          <th>단계</th>
          <th>전통 머신러닝</th>
          <th>딥러닝</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>1. 원본 데이터</td>
          <td>텍스트, 이미지, 표 등</td>
          <td>텍스트, 이미지, 표 등</td>
      </tr>
      <tr>
          <td>2. 분할</td>
          <td>텍스트→토큰, 이미지→픽셀, 음성→프레임</td>
          <td>텍스트→토큰, 이미지→픽셀, 음성→프레임</td>
      </tr>
      <tr>
          <td>3. 벡터화</td>
          <td>숫자로 변환 (ID, RGB, 정규화)</td>
          <td>숫자로 변환 (ID, RGB, 정규화)</td>
      </tr>
      <tr>
          <td>4. Feature 추출</td>
          <td>사람이 설계 (고정)<!-- raw HTML omitted -->- 긍정단어 개수<!-- raw HTML omitted -->- 부정단어 개수<!-- raw HTML omitted -->- 느낌표 개수</td>
          <td>-</td>
      </tr>
      <tr>
          <td>5. 학습 초기화</td>
          <td>랜덤 Parameter</td>
          <td>랜덤 Parameter</td>
      </tr>
      <tr>
          <td>6. 순전파</td>
          <td>Feature × Parameter = 예측</td>
          <td>벡터화된 입력 × Parameter<!-- raw HTML omitted -->→ Layer별 Feature 자동 생성<!-- raw HTML omitted -->→ 예측</td>
      </tr>
      <tr>
          <td>7. 손실 계산</td>
          <td>정답과 비교</td>
          <td>정답과 비교</td>
      </tr>
      <tr>
          <td>8. 역전파</td>
          <td>Parameter 조정</td>
          <td>Parameter 조정<!-- raw HTML omitted -->→ Feature 표현도 변화</td>
      </tr>
      <tr>
          <td>9. 반복</td>
          <td>6<del>8 반복 (수백만</del>수억 회)</td>
          <td>6<del>8 반복 (수백만</del>수억 회)</td>
      </tr>
      <tr>
          <td>10. 학습된 모델</td>
          <td>고정 Feature + 학습된 Parameter</td>
          <td>학습된 Parameter<!-- raw HTML omitted -->(Feature 표현 내장)</td>
      </tr>
      <tr>
          <td>11. 추론: 전처리</td>
          <td>분할 + 벡터화</td>
          <td>분할 + 벡터화</td>
      </tr>
      <tr>
          <td>12. 추론: Feature</td>
          <td>같은 방식으로 Feature 추출</td>
          <td>학습된 모델로 자동 생성</td>
      </tr>
      <tr>
          <td>13. 추론: 예측</td>
          <td>학습된 Parameter로 계산</td>
          <td>학습된 Parameter로 계산</td>
      </tr>
      <tr>
          <td>14. 예측 결과</td>
          <td>출력</td>
          <td>출력</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="2-핵심-용어-완전-정리">2. 핵심 용어 완전 정리<a hidden class="anchor" aria-hidden="true" href="#2-핵심-용어-완전-정리">#</a></h2>
<h3 id="데이터-data">데이터 (Data)<a hidden class="anchor" aria-hidden="true" href="#데이터-data">#</a></h3>
<p>원본 정보</p>
<h4 id="예시">예시<a hidden class="anchor" aria-hidden="true" href="#예시">#</a></h4>
<ul>
<li>텍스트: &ldquo;안녕하세요&rdquo;</li>
<li>이미지: 고양이 사진</li>
<li>표: 집 정보 (평수, 방개수)</li>
</ul>
<h4 id="역할">역할<a hidden class="anchor" aria-hidden="true" href="#역할">#</a></h4>
<p>학습의 재료</p>
<h3 id="분할-segmentation">분할 (Segmentation)<a hidden class="anchor" aria-hidden="true" href="#분할-segmentation">#</a></h3>
<p>데이터를 처리 가능한 작은 단위로 나누는 과정</p>
<h4 id="데이터별-분할">데이터별 분할<a hidden class="anchor" aria-hidden="true" href="#데이터별-분할">#</a></h4>
<ul>
<li>텍스트 → 토큰 (Tokenization)</li>
<li>이미지 → 픽셀 (Pixelation)</li>
<li>음성 → 프레임 (Framing)</li>
</ul>
<h4 id="목적">목적<a hidden class="anchor" aria-hidden="true" href="#목적">#</a></h4>
<p>벡터화하기 전 처리 가능한 단위로 분해</p>
<h3 id="토큰-token">토큰 (Token)<a hidden class="anchor" aria-hidden="true" href="#토큰-token">#</a></h3>
<p>텍스트를 처리 가능한 작은 단위로 나눈 것</p>
<pre tabindex="0"><code>&#34;안녕하세요&#34; → [&#34;안녕&#34;, &#34;하세요&#34;] (2토큰)
&#34;I love you&#34; → [&#34;I&#34;, &#34;love&#34;, &#34;you&#34;] (3토큰)
</code></pre><h4 id="특징">특징<a hidden class="anchor" aria-hidden="true" href="#특징">#</a></h4>
<ul>
<li>NLP(자연어 처리) 전용 용어</li>
<li>이미지는 &ldquo;픽셀&rdquo;, 음성은 &ldquo;프레임&rdquo;</li>
<li>LLM에서 비용/길이 계산 단위</li>
</ul>
<h4 id="범위">범위<a hidden class="anchor" aria-hidden="true" href="#범위">#</a></h4>
<p>자연어 처리 전반 (LLM 전용 아님)</p>
<h3 id="벡터화-vectorization">벡터화 (Vectorization)<a hidden class="anchor" aria-hidden="true" href="#벡터화-vectorization">#</a></h3>
<p>데이터를 숫자로 변환하는 과정 (표현 방식의 변환)</p>
<pre tabindex="0"><code>&#34;안녕&#34; → 1523 (숫자 ID)
빨간 픽셀 → [255, 0, 0] (RGB 값)
</code></pre><h4 id="목적-1">목적<a hidden class="anchor" aria-hidden="true" href="#목적-1">#</a></h4>
<p>기계가 읽을 수 있는 형태로 변환</p>
<h3 id="feature-추출">Feature 추출<a hidden class="anchor" aria-hidden="true" href="#feature-추출">#</a></h3>
<p>벡터화된 데이터 중 의미있는 부분을 선택하거나 새로 만드는 과정</p>
<pre tabindex="0"><code>벡터화: [1523, 8842, ...]
    ↓
Feature 추출: 의미있는 표현 생성
    ↓
Feature: [3, 0, 2]  (긍정단어 개수, 부정단어 개수, 느낌표)
</code></pre><h4 id="핵심">핵심<a hidden class="anchor" aria-hidden="true" href="#핵심">#</a></h4>
<ul>
<li>벡터화는 표현 방식의 변환</li>
<li>Feature 추출은 의미 선택의 과정</li>
<li>둘은 구분되지만 일부 겹침</li>
</ul>
<h3 id="feature-특징">Feature (특징)<a hidden class="anchor" aria-hidden="true" href="#feature-특징">#</a></h3>
<p>모델이 학습에 활용하는 의미있는 숫자 표현</p>
<h4 id="전통-머신러닝-예시-감정-분석">전통 머신러닝 예시 (감정 분석)<a hidden class="anchor" aria-hidden="true" href="#전통-머신러닝-예시-감정-분석">#</a></h4>
<pre tabindex="0"><code>리뷰 텍스트 → Feature (사람이 설계)
- 긍정 단어 개수: 3
- 부정 단어 개수: 0
- 느낌표 개수: 2
→ Feature: [3, 0, 2]
</code></pre><h4 id="딥러닝-예시-감정-분석">딥러닝 예시 (감정 분석)<a hidden class="anchor" aria-hidden="true" href="#딥러닝-예시-감정-분석">#</a></h4>
<pre tabindex="0"><code>리뷰 텍스트 → Feature (자동 학습)
- 입력: &#34;이 영화 정말 최고예요!&#34;
- Layer 1: 단어 조합 패턴
- Layer 2: 감정 강도 패턴
- Layer 3: 전체 의도 (극찬)
</code></pre><h4 id="학습-대상-여부">학습 대상 여부<a hidden class="anchor" aria-hidden="true" href="#학습-대상-여부">#</a></h4>
<ul>
<li>전통 머신러닝: Feature 고정, Parameter만 학습</li>
<li>딥러닝: Parameter 학습을 통해 Feature 표현도 간접 학습</li>
</ul>
<h4 id="핵심-1">핵심<a hidden class="anchor" aria-hidden="true" href="#핵심-1">#</a></h4>
<ul>
<li>모델에 &ldquo;무엇을 볼 것인가&rdquo; 제공</li>
<li>딥러닝: 각 Layer 출력도 Feature</li>
</ul>
<h4 id="전통-머신러닝-vs-딥러닝">전통 머신러닝 vs 딥러닝<a hidden class="anchor" aria-hidden="true" href="#전통-머신러닝-vs-딥러닝">#</a></h4>
<ul>
<li>전통 머신러닝: 사람이 Feature 설계 (고정)</li>
<li>딥러닝: Feature 자동 학습 (Parameter로 제어)</li>
</ul>
<h3 id="parameter-파라미터">Parameter (파라미터)<a hidden class="anchor" aria-hidden="true" href="#parameter-파라미터">#</a></h3>
<p>모델이 학습으로 조정하는 내부 숫자 값</p>
<h4 id="예시-1">예시<a hidden class="anchor" aria-hidden="true" href="#예시-1">#</a></h4>
<pre tabindex="0"><code>감정점수 = (긍정단어 × a) + (부정단어 × b) + (느낌표 × c)

a = 0.5   ← 긍정단어: 양수 (점수 올림)
b = -0.8  ← 부정단어: 음수 (점수 낮춤)
c = 0.3   ← 느낌표: 약한 양수 (약간 긍정)
</code></pre><h4 id="구성">구성<a hidden class="anchor" aria-hidden="true" href="#구성">#</a></h4>
<ul>
<li>가중치 (Weight)</li>
<li>편향 (Bias)</li>
</ul>
<h4 id="핵심-2">핵심<a hidden class="anchor" aria-hidden="true" href="#핵심-2">#</a></h4>
<ul>
<li>학습으로 조정됨 (최적화 대상)</li>
<li>Feature를 &ldquo;어떻게 처리할지&rdquo; 결정</li>
</ul>
<h4 id="크기">크기<a hidden class="anchor" aria-hidden="true" href="#크기">#</a></h4>
<table>
  <thead>
      <tr>
          <th>구분</th>
          <th>Parameter 규모</th>
          <th>예시</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>전통 머신러닝</td>
          <td>10² ~ 10⁴</td>
          <td>선형 회귀, SVM</td>
      </tr>
      <tr>
          <td>CNN (딥러닝)</td>
          <td>10⁵ ~ 10⁷</td>
          <td>이미지넷 모델</td>
      </tr>
      <tr>
          <td>Transformer</td>
          <td>10⁸ ~ 10¹⁰</td>
          <td>BERT, GPT-2</td>
      </tr>
      <tr>
          <td>LLM</td>
          <td>10¹¹ ~ 10¹³</td>
          <td>GPT-3, GPT-4</td>
      </tr>
  </tbody>
</table>
<h3 id="feature-vs-parameter-한눈에">Feature vs Parameter 한눈에<a hidden class="anchor" aria-hidden="true" href="#feature-vs-parameter-한눈에">#</a></h3>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Feature</th>
          <th>Parameter</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>정의</td>
          <td>의미있는 숫자 표현</td>
          <td>학습 가능한 내부 값</td>
      </tr>
      <tr>
          <td>생성 방식</td>
          <td>데이터에서 계산</td>
          <td>랜덤 초기화 후 학습으로 최적화</td>
      </tr>
      <tr>
          <td>역할</td>
          <td>무엇을 볼 것인가</td>
          <td>어떻게 계산할 것인가</td>
      </tr>
      <tr>
          <td>예시</td>
          <td>[3, 0, 2] (긍정, 부정, 느낌표)</td>
          <td>a=0.5, b=-0.8, c=0.3</td>
      </tr>
      <tr>
          <td>위치</td>
          <td>입력/중간 표현</td>
          <td>모델 내부</td>
      </tr>
      <tr>
          <td>학습 대상</td>
          <td>전통 머신러닝: ✗<!-- raw HTML omitted -->딥러닝: 간접 ✓</td>
          <td>✓ (직접 최적화)</td>
      </tr>
      <tr>
          <td>비유</td>
          <td>요리 재료</td>
          <td>요리 레시피</td>
      </tr>
  </tbody>
</table>
<h4 id="관계">관계<a hidden class="anchor" aria-hidden="true" href="#관계">#</a></h4>
<pre tabindex="0"><code>Feature (입력) × Parameter (가중치) = 출력 (예측)
</code></pre><ul>
<li>학습: 출력 → 손실 계산 → Parameter 조정 → 반복</li>
<li>추론: 출력 = 최종 예측</li>
</ul>
<h3 id="학습-training">학습 (Training)<a hidden class="anchor" aria-hidden="true" href="#학습-training">#</a></h3>
<p>데이터로 최적의 Parameter를 찾는 과정</p>
<ol>
<li>랜덤 Parameter로 시작</li>
<li>Feature × Parameter = 예측</li>
<li>오차 계산 (틀린 정도)</li>
<li>역전파로 Parameter 조정</li>
<li>반복 (점점 정확해짐)</li>
</ol>
<h4 id="결과">결과<a hidden class="anchor" aria-hidden="true" href="#결과">#</a></h4>
<p>학습된 모델 (구조 + 최적화된 Parameter)</p>
<h4 id="딥러닝에서">딥러닝에서<a hidden class="anchor" aria-hidden="true" href="#딥러닝에서">#</a></h4>
<p>Parameter 학습으로 Feature 표현도 함께 최적화됨</p>
<h3 id="모델-model">모델 (Model)<a hidden class="anchor" aria-hidden="true" href="#모델-model">#</a></h3>
<p>구조(Architecture) + 학습된 Parameter</p>
<pre tabindex="0"><code>모델 파일 (예: PyTorch .pt, TensorFlow .pb):
- 네트워크 구조 정의
- 학습된 Parameter 값들
</code></pre><h4 id="예시-2">예시<a hidden class="anchor" aria-hidden="true" href="#예시-2">#</a></h4>
<pre tabindex="0"><code>GPT-3 모델 = Transformer 구조 + 1,750억 개 Parameter = 700GB
</code></pre><h3 id="추론-inference">추론 (Inference)<a hidden class="anchor" aria-hidden="true" href="#추론-inference">#</a></h3>
<p>학습된 모델로 예측하는 과정</p>
<pre tabindex="0"><code>입력 Feature × 학습된 Parameter = 예측 결과
</code></pre><h4 id="예시-3">예시<a hidden class="anchor" aria-hidden="true" href="#예시-3">#</a></h4>
<ul>
<li>새 리뷰: &ldquo;음식 맛있어요!&rdquo; → Feature 추출 → 모델로 계산 → &ldquo;긍정 92%&rdquo;</li>
<li>새 리뷰: &ldquo;별로예요&hellip;&rdquo; → Feature 추출 → 모델로 계산 → &ldquo;부정 85%&rdquo;</li>
</ul>
<h4 id="사용">사용<a hidden class="anchor" aria-hidden="true" href="#사용">#</a></h4>
<p>새 입력 → 모델 → 예측</p>
<hr>
<h2 id="3-전통-머신러닝-vs-딥러닝-비교">3. 전통 머신러닝 vs 딥러닝 비교<a hidden class="anchor" aria-hidden="true" href="#3-전통-머신러닝-vs-딥러닝-비교">#</a></h2>
<h3 id="전통-머신러닝">전통 머신러닝<a hidden class="anchor" aria-hidden="true" href="#전통-머신러닝">#</a></h3>
<ul>
<li>Feature: 사람 설계 (고정)</li>
<li>학습: Parameter만</li>
<li>예시: 선형 회귀, SVM, 랜덤포레스트</li>
</ul>
<h3 id="딥러닝">딥러닝<a hidden class="anchor" aria-hidden="true" href="#딥러닝">#</a></h3>
<ul>
<li>Feature: 학습 중 자동 생성</li>
<li>학습: Parameter + Feature 동시</li>
<li>예시: CNN, RNN, Transformer, LLM</li>
</ul>
<h3 id="비교표">비교표<a hidden class="anchor" aria-hidden="true" href="#비교표">#</a></h3>
<table>
  <thead>
      <tr>
          <th></th>
          <th>전통 머신러닝</th>
          <th>딥러닝</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Feature</td>
          <td>사람이 설계 (고정)</td>
          <td>Parameter 학습으로 자동 학습</td>
      </tr>
      <tr>
          <td>Parameter</td>
          <td>자동 학습</td>
          <td>자동 학습</td>
      </tr>
      <tr>
          <td>파라미터 수</td>
          <td>10² ~ 10⁴</td>
          <td>10⁵ ~ 10¹³</td>
      </tr>
      <tr>
          <td>계산량</td>
          <td>적음</td>
          <td>많음 (GPU 필요)</td>
      </tr>
      <tr>
          <td>데이터 필요량</td>
          <td>수천~수만</td>
          <td>수백만~수조</td>
      </tr>
      <tr>
          <td>학습 시간</td>
          <td>분~시간</td>
          <td>일~월</td>
      </tr>
      <tr>
          <td>복잡도</td>
          <td>단순 패턴</td>
          <td>복잡한 패턴</td>
      </tr>
      <tr>
          <td>전처리</td>
          <td>필요</td>
          <td>필요 (자동≠불필요)</td>
      </tr>
      <tr>
          <td>해석 가능성</td>
          <td>높음</td>
          <td>상대적으로 낮음 (XAI로 개선 중)</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="4-전체-요약">4. 전체 요약<a hidden class="anchor" aria-hidden="true" href="#4-전체-요약">#</a></h2>
<h3 id="머신러닝--데이터로-모델-만들기">머신러닝 = 데이터로 모델 만들기<a hidden class="anchor" aria-hidden="true" href="#머신러닝--데이터로-모델-만들기">#</a></h3>
<pre tabindex="0"><code>데이터 → Feature → Parameter 학습 → 모델 → 예측
</code></pre><h3 id="전통-ml-vs-딥러닝">전통 ML vs 딥러닝<a hidden class="anchor" aria-hidden="true" href="#전통-ml-vs-딥러닝">#</a></h3>
<ul>
<li>전통 ML: Feature 사람 설계</li>
<li>딥러닝: Feature 자동 학습</li>
</ul>
<h3 id="발전-과정">발전 과정<a hidden class="anchor" aria-hidden="true" href="#발전-과정">#</a></h3>
<h4 id="2000년대-전통-머신러닝">2000년대: 전통 머신러닝<a hidden class="anchor" aria-hidden="true" href="#2000년대-전통-머신러닝">#</a></h4>
<ul>
<li>사람이 Feature 설계</li>
<li>Parameter 10² ~ 10⁴ 개</li>
<li>간단한 작업</li>
</ul>
<h4 id="2010년대-딥러닝">2010년대: 딥러닝<a hidden class="anchor" aria-hidden="true" href="#2010년대-딥러닝">#</a></h4>
<ul>
<li>Feature 자동 학습</li>
<li>Parameter 10⁵ ~ 10⁷ 개</li>
<li>이미지, 음성 인식 성공</li>
<li>GPU 필수</li>
</ul>
<h4 id="2020년대-llm">2020년대: LLM<a hidden class="anchor" aria-hidden="true" href="#2020년대-llm">#</a></h4>
<ul>
<li>초대규모 Parameter (10¹¹ ~ 10¹³)</li>
<li>언어 이해/생성</li>
<li>Pre-training + Fine-tuning</li>
<li>범용 AI의 시작</li>
</ul>
<hr>
<h2 id="5-실무-관점-정리">5. 실무 관점 정리<a hidden class="anchor" aria-hidden="true" href="#5-실무-관점-정리">#</a></h2>
<h3 id="ai-도입-시-이해해야-할-것">AI 도입 시 이해해야 할 것<a hidden class="anchor" aria-hidden="true" href="#ai-도입-시-이해해야-할-것">#</a></h3>
<h4 id="전통-머신러닝을-선택하는-경우">전통 머신러닝을 선택하는 경우<a hidden class="anchor" aria-hidden="true" href="#전통-머신러닝을-선택하는-경우">#</a></h4>
<ul>
<li>정형 데이터 (표 형태)</li>
<li>데이터: 수천~수만 건</li>
<li>해석 가능성 중요</li>
<li>빠른 개발 필요</li>
<li>예: 집값 예측, 고객 이탈 예측</li>
</ul>
<h4 id="딥러닝을-선택하는-경우">딥러닝을 선택하는 경우<a hidden class="anchor" aria-hidden="true" href="#딥러닝을-선택하는-경우">#</a></h4>
<ul>
<li>비정형 데이터 (이미지, 음성, 텍스트)</li>
<li>데이터: 수십만 건 이상</li>
<li>복잡한 패턴</li>
<li>GPU 인프라 있음</li>
<li>예: 얼굴 인식, 음성 인식</li>
</ul>
<h4 id="llm-api-사용하는-경우">LLM API 사용하는 경우<a hidden class="anchor" aria-hidden="true" href="#llm-api-사용하는-경우">#</a></h4>
<ul>
<li>텍스트 처리</li>
<li>빠른 프로토타입</li>
<li>자체 모델 학습 불필요</li>
<li>예: 챗봇, 요약, 번역</li>
</ul>
<h3 id="준비-사항-체크리스트">준비 사항 체크리스트<a hidden class="anchor" aria-hidden="true" href="#준비-사항-체크리스트">#</a></h3>
<ul>
<li><input disabled="" type="checkbox"> 데이터 충분한가? (양, 품질)</li>
<li><input disabled="" type="checkbox"> 리소스 있는가? (GPU, 시간, 비용)</li>
<li><input disabled="" type="checkbox"> Feature 설계 가능한가? (전통 ML)</li>
<li><input disabled="" type="checkbox"> 해석 가능성 필요한가?</li>
<li><input disabled="" type="checkbox"> 실시간 응답 필요한가?</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://dont.kr/tags/ai/">Ai</a></li>
      <li><a href="https://dont.kr/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://dont.kr/tags/deep-learning/">Deep Learning</a></li>
    </ul>
  </footer><script src="https://giscus.app/client.js"
    data-repo="duddns/dontkr-www"
    data-repo-id="R_kgDOJ5uOLg"
    data-category="General"
    data-category-id="DIC_kwDOJ5uOLs4CzVRe"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="preferred_color_scheme"
    data-lang="ko"
    crossorigin="anonymous"
    async>
</script>

</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://dont.kr/">DoNT - Do Not Think!!!</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
