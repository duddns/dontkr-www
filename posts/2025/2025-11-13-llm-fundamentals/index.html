<!DOCTYPE html>
<html lang="ko" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LLM 개념 정리 | DoNT - Do Not Think!!!</title>
<meta name="keywords" content="ai, llm, transformer">
<meta name="description" content="1. LLM이란 무엇인가?
정의
Large Language Model = 대규모 언어 모델

엄청난 양의 텍스트로 학습한
파라미터가 수백억~조 개인
언어를 이해하고 생성하는 AI 모델

본질
&ldquo;다음 단어 예측&quot;을 극한까지 잘하는 모델
입력: &#34;오늘 날씨가 정말&#34;
LLM: &#34;좋네요&#34; (가장 자연스러운 다음 단어 선택)
→ 이 단순한 원리로 대화, 요약, 번역, 코딩까지 가능

2. LLM = 딥러닝의 한 종류
AI 기술 계층도
AI
└─ 머신러닝
    ├─ 전통 머신러닝
    └─ 딥러닝
        └─ Transformer
            └─ LLM ← 여기
LLM의 위치

AI의 일부
딥러닝의 일부
Transformer 기반
현재 가장 주목받는 분야

Multimodal 확장
최근 트렌드:">
<meta name="author" content="">
<link rel="canonical" href="https://dont.kr/posts/2025/2025-11-13-llm-fundamentals/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.4bef824c135d95ffd8df119b1b511d3c1507c4d3fed7cf470539d03f95ec7625.css" integrity="sha256-S&#43;&#43;CTBNdlf/Y3xGbG1EdPBUHxNP&#43;189HBTnQP5XsdiU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://dont.kr/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://dont.kr/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://dont.kr/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://dont.kr/apple-touch-icon.png">
<link rel="mask-icon" href="https://dont.kr/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="ko" href="https://dont.kr/posts/2025/2025-11-13-llm-fundamentals/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://dont.kr/posts/2025/2025-11-13-llm-fundamentals/">
  <meta property="og:site_name" content="DoNT - Do Not Think!!!">
  <meta property="og:title" content="LLM 개념 정리">
  <meta property="og:description" content="1. LLM이란 무엇인가? 정의 Large Language Model = 대규모 언어 모델
엄청난 양의 텍스트로 학습한 파라미터가 수백억~조 개인 언어를 이해하고 생성하는 AI 모델 본질 “다음 단어 예측&#34;을 극한까지 잘하는 모델
입력: &#34;오늘 날씨가 정말&#34; LLM: &#34;좋네요&#34; (가장 자연스러운 다음 단어 선택) → 이 단순한 원리로 대화, 요약, 번역, 코딩까지 가능
2. LLM = 딥러닝의 한 종류 AI 기술 계층도 AI └─ 머신러닝 ├─ 전통 머신러닝 └─ 딥러닝 └─ Transformer └─ LLM ← 여기 LLM의 위치 AI의 일부 딥러닝의 일부 Transformer 기반 현재 가장 주목받는 분야 Multimodal 확장 최근 트렌드:">
  <meta property="og:locale" content="ko-kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-13T00:00:00+09:00">
    <meta property="article:modified_time" content="2025-11-13T00:00:00+09:00">
    <meta property="article:tag" content="Ai">
    <meta property="article:tag" content="Llm">
    <meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LLM 개념 정리">
<meta name="twitter:description" content="1. LLM이란 무엇인가?
정의
Large Language Model = 대규모 언어 모델

엄청난 양의 텍스트로 학습한
파라미터가 수백억~조 개인
언어를 이해하고 생성하는 AI 모델

본질
&ldquo;다음 단어 예측&quot;을 극한까지 잘하는 모델
입력: &#34;오늘 날씨가 정말&#34;
LLM: &#34;좋네요&#34; (가장 자연스러운 다음 단어 선택)
→ 이 단순한 원리로 대화, 요약, 번역, 코딩까지 가능

2. LLM = 딥러닝의 한 종류
AI 기술 계층도
AI
└─ 머신러닝
    ├─ 전통 머신러닝
    └─ 딥러닝
        └─ Transformer
            └─ LLM ← 여기
LLM의 위치

AI의 일부
딥러닝의 일부
Transformer 기반
현재 가장 주목받는 분야

Multimodal 확장
최근 트렌드:">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://dont.kr/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM 개념 정리",
      "item": "https://dont.kr/posts/2025/2025-11-13-llm-fundamentals/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM 개념 정리",
  "name": "LLM 개념 정리",
  "description": "1. LLM이란 무엇인가? 정의 Large Language Model = 대규모 언어 모델\n엄청난 양의 텍스트로 학습한 파라미터가 수백억~조 개인 언어를 이해하고 생성하는 AI 모델 본질 \u0026ldquo;다음 단어 예측\u0026quot;을 극한까지 잘하는 모델\n입력: \u0026#34;오늘 날씨가 정말\u0026#34; LLM: \u0026#34;좋네요\u0026#34; (가장 자연스러운 다음 단어 선택) → 이 단순한 원리로 대화, 요약, 번역, 코딩까지 가능\n2. LLM = 딥러닝의 한 종류 AI 기술 계층도 AI └─ 머신러닝 ├─ 전통 머신러닝 └─ 딥러닝 └─ Transformer └─ LLM ← 여기 LLM의 위치 AI의 일부 딥러닝의 일부 Transformer 기반 현재 가장 주목받는 분야 Multimodal 확장 최근 트렌드:\n",
  "keywords": [
    "ai", "llm", "transformer"
  ],
  "articleBody": "1. LLM이란 무엇인가? 정의 Large Language Model = 대규모 언어 모델\n엄청난 양의 텍스트로 학습한 파라미터가 수백억~조 개인 언어를 이해하고 생성하는 AI 모델 본질 “다음 단어 예측\"을 극한까지 잘하는 모델\n입력: \"오늘 날씨가 정말\" LLM: \"좋네요\" (가장 자연스러운 다음 단어 선택) → 이 단순한 원리로 대화, 요약, 번역, 코딩까지 가능\n2. LLM = 딥러닝의 한 종류 AI 기술 계층도 AI └─ 머신러닝 ├─ 전통 머신러닝 └─ 딥러닝 └─ Transformer └─ LLM ← 여기 LLM의 위치 AI의 일부 딥러닝의 일부 Transformer 기반 현재 가장 주목받는 분야 Multimodal 확장 최근 트렌드:\nLLM (텍스트) + Vision (이미지) + Audio (음성) = Multimodal AI (GPT-4V, Gemini) → 하지만 코어는 여전히 LLM\n3. LLM의 핵심 특징 Large (대규모 Parameter) 규모: 10¹¹ ~ 10¹³ 개\nGPT-3: 1.75 × 10¹¹ 개 (700GB) GPT-4: 추정 10¹² ~ 10¹³ 개 Claude 3.5: 비공개 (아마 수천억~조) 많을수록:\n복잡한 패턴 학습 미묘한 뉘앙스 이해 추론 능력 향상 Language (언어 전용) 텍스트만 다룸:\n입력: 텍스트 처리: 토큰 출력: 텍스트 학습 데이터: 텍스트 이미지/음성 모델과 구분되는 핵심 Model (학습된 Parameter) 머신러닝으로 학습된 결과물:\n사람이 규칙을 짜지 않음 데이터에서 스스로 패턴 학습 사전학습: “다음 토큰 예측” 미세조정: SFT + RLHF 한번 학습하면 계속 사용 → 대화, 요약, 번역, 코딩 모두 가능 4. LLM 학습 학습 과정 ━━━━━━━━━━━━━━━━━━━━━━━ 1단계: Pre-training (사전학습) ━━━━━━━━━━━━━━━━━━━━━━━ 대규모 텍스트 데이터 (수조 토큰) ↓ 토큰화 \"안녕하세요\" → [\"안녕\", \"하세요\"] ↓ 임베딩 (Feature 추출) [[0.2, -0.5, 0.8], [0.1, 0.3, -0.2]] ↓ ━━━━━━━━━━━━━━━━━━━━━━━ Transformer Layer 반복 ━━━━━━━━━━━━━━━━━━━━━━━ ↓ [Layer 1] Feature → 다음 Feature [Layer 2] Feature → 다음 Feature ... [마지막 Layer] Feature → 최종 표현 ↓ \"다음 토큰 확률 예측\" 반복 학습 ↓ 10¹¹ ~ 10¹³ 개 Parameter 최적화 ↓ ━━━━━━━━━━━━━━━━━━━━━━━ 사전학습 모델 완성 ━━━━━━━━━━━━━━━━━━━━━━━ ↓ ━━━━━━━━━━━━━━━━━━━━━━━ 2단계: Fine-tuning (미세조정) ━━━━━━━━━━━━━━━━━━━━━━━ ↓ ① Supervised Fine-tuning (SFT) - 고품질 대화 데이터로 학습 - 지시 따르기 능력 향상 ↓ ② RLHF (Reinforcement Learning from Human Feedback, 인간 피드백 강화학습) - 사람이 \"좋은 답변\" 평가 - 유용성, 안전성 향상 ↓ ━━━━━━━━━━━━━━━━━━━━━━━ 대화형 LLM 완성 ━━━━━━━━━━━━━━━━━━━━━━━ ↓ GPT, Claude 등 학습 핵심 기본: 다음 토큰 확률 예측 학습 완성: SFT + RLHF로 대화형 모델 발전 5. LLM 추론 추론 과정 \"오늘 날씨가\" ↓ 토큰화: [\"오늘\", \"날씨\", \"가\"] ↓ 임베딩: Feature 추출 ↓ 수십~수백개 Layer 통과 (각 Layer마다 새로운 Feature 생성) ↓ 다음 토큰 확률 분포 계산 \"좋네요\" (70%), \"나쁘네요\" (20%), \"흐리네요\" (8%), ... ↓ Sampling 전략에 따라 선택 ↓ \"좋네요\" 선택 추론 메커니즘 확률 분포 계산 후 Sampling 전략에 따라 토큰 선택 Temperature: 확률 분포 조절 값 낮음 (0~0.5): 보수적 (높은 확률만) 기본 (1.0): 균형 높음 (1.5~2.0): 창의적 (다양한 선택) top-k, top-p 등도 사용 6. LLM은 왜 똑똑할까? Transformer 구조의 비밀 Transformer 란? 2017년 등장한 딥러닝 구조 LLM의 핵심 기술 문맥을 이해하는 능력이 탁월 핵심 능력: 문맥 파악 (Attention 메커니즘) \"철수는 학교에 갔다. 그는 친구를 만났다.\" ↑ ↑ └─── 관계 파악 ───┘ 문장의 모든 단어 관계를 동시에 분석 “그는” 이 “철수” 를 가리킨다고 이해 각 Layer 마다 Attention 메커니즘 적용 층층이 쌓인 학습 (Multi-layer 구조) 입력 텍스트 ↓ [Layer 1] 간단한 패턴 (단어 조합) ↓ [Layer 2] 복잡한 패턴 (문법, 의미) ↓ [Layer 3] 추상적 개념 (의도, 감정) ↓ 출력 텍스트 결과 긴 문맥도 이해 미묘한 뉘앙스 파악 논리적 추론 가능 7. LLM이 잘하는 것 자연스러운 대화 문맥 유지 의도 파악 자연스러운 응답 텍스트 생성/변환 요약 번역 다시 쓰기 형식 변환 지식 활용 질문 답변 설명 조언 추론 논리적 사고 단계별 분석 문제 해결 창작 글쓰기 아이디어 발산 스토리텔링 코딩 코드 생성 버그 수정 설명 8. LLM의 한계 할루시네이션 (가장 큰 문제) 사실이 아닌 내용을 그럴듯하게 생성\n질문: \"2025년 노벨물리학상 수상자는?\" LLM: \"김철수 박사입니다\" (지어낸 답) → 자신 없어도 확신하는 척\n→ 중요한 사실은 반드시 검증 필요\n지식 컷오프 학습 시점 이후 정보 모름\nClaude: 2025년 1월까지만 앎 최신 뉴스/사건 모름 검색 도구로 보완 가능 계산 약함 수학 계산 실수 가능\n질문: \"123456 × 789 = ?\" LLM: 틀릴 수 있음 → 복잡한 계산은 코드/도구 사용\n일관성 문제 같은 질문에 다른 답변 긴 대화에서 앞뒤 모순 편향 학습 데이터의 편향 반영 문화/성별/인종 편견 가능 비용 API 호출마다 과금 대량 사용 시 비용 부담 9. 핵심 요약 LLM이란? 엄청난 텍스트로 학습한, 언어를 이해하고 생성하는 대규모 AI 모델 딥러닝 \u003e Transformer 기반 핵심 특징 Large: 수백억~조 개 파라미터 Language: 텍스트만 처리 Model: 학습된 Parameter 작동 원리 다음 토큰 확률 예측이 핵심 Transformer 구조 (Attention 메커니즘) 토큰 단위 처리 학습 과정 Pre-training: 다음 토큰 예측 학습 Fine-tuning: SFT + RLHF로 대화형 발전 잘하는 것 대화, 요약, 번역, 추론, 창작, 코딩 한계 할루시네이션, 지식 컷오프, 계산 약함, 비용 ",
  "wordCount" : "729",
  "inLanguage": "ko",
  "datePublished": "2025-11-13T00:00:00+09:00",
  "dateModified": "2025-11-13T00:00:00+09:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dont.kr/posts/2025/2025-11-13-llm-fundamentals/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "DoNT - Do Not Think!!!",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dont.kr/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://dont.kr/" accesskey="h" title="DoNT - Do Not Think!!! (Alt + H)">DoNT - Do Not Think!!!</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://dont.kr/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://dont.kr/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      LLM 개념 정리
    </h1>
    <div class="post-meta"><span title='2025-11-13 00:00:00 +0900 KST'>2025년 11월 13일</span>

</div>
  </header> 
  <div class="post-content"><h2 id="1-llm이란-무엇인가">1. LLM이란 무엇인가?<a hidden class="anchor" aria-hidden="true" href="#1-llm이란-무엇인가">#</a></h2>
<h3 id="정의">정의<a hidden class="anchor" aria-hidden="true" href="#정의">#</a></h3>
<p>Large Language Model = 대규모 언어 모델</p>
<ul>
<li>엄청난 양의 텍스트로 학습한</li>
<li>파라미터가 수백억~조 개인</li>
<li>언어를 이해하고 생성하는 AI 모델</li>
</ul>
<h3 id="본질">본질<a hidden class="anchor" aria-hidden="true" href="#본질">#</a></h3>
<p>&ldquo;다음 단어 예측&quot;을 극한까지 잘하는 모델</p>
<pre tabindex="0"><code>입력: &#34;오늘 날씨가 정말&#34;
LLM: &#34;좋네요&#34; (가장 자연스러운 다음 단어 선택)
</code></pre><p>→ 이 단순한 원리로 대화, 요약, 번역, 코딩까지 가능</p>
<hr>
<h2 id="2-llm--딥러닝의-한-종류">2. LLM = 딥러닝의 한 종류<a hidden class="anchor" aria-hidden="true" href="#2-llm--딥러닝의-한-종류">#</a></h2>
<h3 id="ai-기술-계층도">AI 기술 계층도<a hidden class="anchor" aria-hidden="true" href="#ai-기술-계층도">#</a></h3>
<pre tabindex="0"><code>AI
└─ 머신러닝
    ├─ 전통 머신러닝
    └─ 딥러닝
        └─ Transformer
            └─ LLM ← 여기
</code></pre><h3 id="llm의-위치">LLM의 위치<a hidden class="anchor" aria-hidden="true" href="#llm의-위치">#</a></h3>
<ul>
<li>AI의 일부</li>
<li>딥러닝의 일부</li>
<li>Transformer 기반</li>
<li>현재 가장 주목받는 분야</li>
</ul>
<h3 id="multimodal-확장">Multimodal 확장<a hidden class="anchor" aria-hidden="true" href="#multimodal-확장">#</a></h3>
<p>최근 트렌드:</p>
<pre tabindex="0"><code>LLM (텍스트)
    +
Vision (이미지)
    +
Audio (음성)
    =
Multimodal AI (GPT-4V, Gemini)
</code></pre><p>→ 하지만 코어는 여전히 LLM</p>
<hr>
<h2 id="3-llm의-핵심-특징">3. LLM의 핵심 특징<a hidden class="anchor" aria-hidden="true" href="#3-llm의-핵심-특징">#</a></h2>
<h3 id="large-대규모-parameter">Large (대규모 Parameter)<a hidden class="anchor" aria-hidden="true" href="#large-대규모-parameter">#</a></h3>
<p><strong>규모:</strong> 10¹¹ ~ 10¹³ 개</p>
<ul>
<li>GPT-3: 1.75 × 10¹¹ 개 (700GB)</li>
<li>GPT-4: 추정 10¹² ~ 10¹³ 개</li>
<li>Claude 3.5: 비공개 (아마 수천억~조)</li>
</ul>
<p><strong>많을수록:</strong></p>
<ul>
<li>복잡한 패턴 학습</li>
<li>미묘한 뉘앙스 이해</li>
<li>추론 능력 향상</li>
</ul>
<h3 id="language-언어-전용">Language (언어 전용)<a hidden class="anchor" aria-hidden="true" href="#language-언어-전용">#</a></h3>
<p><strong>텍스트만 다룸:</strong></p>
<ul>
<li>입력: 텍스트</li>
<li>처리: 토큰</li>
<li>출력: 텍스트</li>
<li>학습 데이터: 텍스트</li>
<li>이미지/음성 모델과 구분되는 핵심</li>
</ul>
<h3 id="model-학습된-parameter">Model (학습된 Parameter)<a hidden class="anchor" aria-hidden="true" href="#model-학습된-parameter">#</a></h3>
<p><strong>머신러닝으로 학습된 결과물:</strong></p>
<ul>
<li>사람이 규칙을 짜지 않음</li>
<li>데이터에서 스스로 패턴 학습</li>
<li>사전학습: &ldquo;다음 토큰 예측&rdquo;</li>
<li>미세조정: SFT + RLHF</li>
<li>한번 학습하면 계속 사용</li>
<li>→ 대화, 요약, 번역, 코딩 모두 가능</li>
</ul>
<hr>
<h2 id="4-llm-학습">4. LLM 학습<a hidden class="anchor" aria-hidden="true" href="#4-llm-학습">#</a></h2>
<h3 id="학습-과정">학습 과정<a hidden class="anchor" aria-hidden="true" href="#학습-과정">#</a></h3>
<pre tabindex="0"><code>━━━━━━━━━━━━━━━━━━━━━━━
 1단계: Pre-training (사전학습)
━━━━━━━━━━━━━━━━━━━━━━━

대규모 텍스트 데이터 (수조 토큰)
    ↓
토큰화
&#34;안녕하세요&#34; → [&#34;안녕&#34;, &#34;하세요&#34;]
    ↓
임베딩 (Feature 추출)
[[0.2, -0.5, 0.8], [0.1, 0.3, -0.2]]
    ↓
━━━━━━━━━━━━━━━━━━━━━━━
 Transformer Layer 반복
━━━━━━━━━━━━━━━━━━━━━━━
    ↓
[Layer 1] Feature → 다음 Feature
[Layer 2] Feature → 다음 Feature
...
[마지막 Layer] Feature → 최종 표현
    ↓
&#34;다음 토큰 확률 예측&#34; 반복 학습
    ↓
10¹¹ ~ 10¹³ 개 Parameter 최적화
    ↓
━━━━━━━━━━━━━━━━━━━━━━━
 사전학습 모델 완성
━━━━━━━━━━━━━━━━━━━━━━━
    ↓
━━━━━━━━━━━━━━━━━━━━━━━
 2단계: Fine-tuning (미세조정)
━━━━━━━━━━━━━━━━━━━━━━━
    ↓
① Supervised Fine-tuning (SFT)
   - 고품질 대화 데이터로 학습
   - 지시 따르기 능력 향상
    ↓
② RLHF (Reinforcement Learning from Human Feedback, 인간 피드백 강화학습)
   - 사람이 &#34;좋은 답변&#34; 평가
   - 유용성, 안전성 향상
    ↓
━━━━━━━━━━━━━━━━━━━━━━━
 대화형 LLM 완성
━━━━━━━━━━━━━━━━━━━━━━━
    ↓
GPT, Claude 등
</code></pre><h3 id="학습-핵심">학습 핵심<a hidden class="anchor" aria-hidden="true" href="#학습-핵심">#</a></h3>
<ul>
<li>기본: 다음 토큰 확률 예측 학습</li>
<li>완성: SFT + RLHF로 대화형 모델 발전</li>
</ul>
<hr>
<h2 id="5-llm-추론">5. LLM 추론<a hidden class="anchor" aria-hidden="true" href="#5-llm-추론">#</a></h2>
<h3 id="추론-과정">추론 과정<a hidden class="anchor" aria-hidden="true" href="#추론-과정">#</a></h3>
<pre tabindex="0"><code>&#34;오늘 날씨가&#34;
    ↓
토큰화: [&#34;오늘&#34;, &#34;날씨&#34;, &#34;가&#34;]
    ↓
임베딩: Feature 추출
    ↓
수십~수백개 Layer 통과
(각 Layer마다 새로운 Feature 생성)
    ↓
다음 토큰 확률 분포 계산
&#34;좋네요&#34; (70%), &#34;나쁘네요&#34; (20%), &#34;흐리네요&#34; (8%), ...
    ↓
Sampling 전략에 따라 선택
    ↓
&#34;좋네요&#34; 선택
</code></pre><h3 id="추론-메커니즘">추론 메커니즘<a hidden class="anchor" aria-hidden="true" href="#추론-메커니즘">#</a></h3>
<ul>
<li>확률 분포 계산 후</li>
<li>Sampling 전략에 따라 토큰 선택
<ul>
<li>Temperature: 확률 분포 조절 값</li>
<li>낮음 (0~0.5): 보수적 (높은 확률만)</li>
<li>기본 (1.0): 균형</li>
<li>높음 (1.5~2.0): 창의적 (다양한 선택)</li>
<li>top-k, top-p 등도 사용</li>
</ul>
</li>
</ul>
<hr>
<h2 id="6-llm은-왜-똑똑할까">6. LLM은 왜 똑똑할까?<a hidden class="anchor" aria-hidden="true" href="#6-llm은-왜-똑똑할까">#</a></h2>
<h3 id="transformer-구조의-비밀">Transformer 구조의 비밀<a hidden class="anchor" aria-hidden="true" href="#transformer-구조의-비밀">#</a></h3>
<h4 id="transformer-란">Transformer 란?<a hidden class="anchor" aria-hidden="true" href="#transformer-란">#</a></h4>
<ul>
<li>2017년 등장한 딥러닝 구조</li>
<li>LLM의 핵심 기술</li>
<li>문맥을 이해하는 능력이 탁월</li>
</ul>
<h4 id="핵심-능력-문맥-파악-attention-메커니즘">핵심 능력: 문맥 파악 (Attention 메커니즘)<a hidden class="anchor" aria-hidden="true" href="#핵심-능력-문맥-파악-attention-메커니즘">#</a></h4>
<pre tabindex="0"><code>&#34;철수는 학교에 갔다. 그는 친구를 만났다.&#34;
 ↑               ↑
 └─── 관계 파악 ───┘
</code></pre><ul>
<li>문장의 모든 단어 관계를 동시에 분석</li>
<li>&ldquo;그는&rdquo; 이 &ldquo;철수&rdquo; 를 가리킨다고 이해</li>
<li>각 Layer 마다 Attention 메커니즘 적용</li>
</ul>
<h4 id="층층이-쌓인-학습-multi-layer-구조">층층이 쌓인 학습 (Multi-layer 구조)<a hidden class="anchor" aria-hidden="true" href="#층층이-쌓인-학습-multi-layer-구조">#</a></h4>
<pre tabindex="0"><code>입력 텍스트
    ↓
[Layer 1] 간단한 패턴 (단어 조합)
    ↓
[Layer 2] 복잡한 패턴 (문법, 의미)
    ↓
[Layer 3] 추상적 개념 (의도, 감정)
    ↓
출력 텍스트
</code></pre><h4 id="결과">결과<a hidden class="anchor" aria-hidden="true" href="#결과">#</a></h4>
<ul>
<li>긴 문맥도 이해</li>
<li>미묘한 뉘앙스 파악</li>
<li>논리적 추론 가능</li>
</ul>
<hr>
<h2 id="7-llm이-잘하는-것">7. LLM이 잘하는 것<a hidden class="anchor" aria-hidden="true" href="#7-llm이-잘하는-것">#</a></h2>
<h3 id="자연스러운-대화">자연스러운 대화<a hidden class="anchor" aria-hidden="true" href="#자연스러운-대화">#</a></h3>
<ul>
<li>문맥 유지</li>
<li>의도 파악</li>
<li>자연스러운 응답</li>
</ul>
<h3 id="텍스트-생성변환">텍스트 생성/변환<a hidden class="anchor" aria-hidden="true" href="#텍스트-생성변환">#</a></h3>
<ul>
<li>요약</li>
<li>번역</li>
<li>다시 쓰기</li>
<li>형식 변환</li>
</ul>
<h3 id="지식-활용">지식 활용<a hidden class="anchor" aria-hidden="true" href="#지식-활용">#</a></h3>
<ul>
<li>질문 답변</li>
<li>설명</li>
<li>조언</li>
</ul>
<h3 id="추론">추론<a hidden class="anchor" aria-hidden="true" href="#추론">#</a></h3>
<ul>
<li>논리적 사고</li>
<li>단계별 분석</li>
<li>문제 해결</li>
</ul>
<h3 id="창작">창작<a hidden class="anchor" aria-hidden="true" href="#창작">#</a></h3>
<ul>
<li>글쓰기</li>
<li>아이디어 발산</li>
<li>스토리텔링</li>
</ul>
<h3 id="코딩">코딩<a hidden class="anchor" aria-hidden="true" href="#코딩">#</a></h3>
<ul>
<li>코드 생성</li>
<li>버그 수정</li>
<li>설명</li>
</ul>
<hr>
<h2 id="8-llm의-한계">8. LLM의 한계<a hidden class="anchor" aria-hidden="true" href="#8-llm의-한계">#</a></h2>
<h3 id="할루시네이션-가장-큰-문제">할루시네이션 (가장 큰 문제)<a hidden class="anchor" aria-hidden="true" href="#할루시네이션-가장-큰-문제">#</a></h3>
<p>사실이 아닌 내용을 그럴듯하게 생성</p>
<pre tabindex="0"><code>질문: &#34;2025년 노벨물리학상 수상자는?&#34;
LLM: &#34;김철수 박사입니다&#34; (지어낸 답)
</code></pre><p>→ 자신 없어도 확신하는 척<br>
→ 중요한 사실은 반드시 검증 필요</p>
<h3 id="지식-컷오프">지식 컷오프<a hidden class="anchor" aria-hidden="true" href="#지식-컷오프">#</a></h3>
<p>학습 시점 이후 정보 모름</p>
<ul>
<li>Claude: 2025년 1월까지만 앎</li>
<li>최신 뉴스/사건 모름</li>
<li>검색 도구로 보완 가능</li>
</ul>
<h3 id="계산-약함">계산 약함<a hidden class="anchor" aria-hidden="true" href="#계산-약함">#</a></h3>
<p>수학 계산 실수 가능</p>
<pre tabindex="0"><code>질문: &#34;123456 × 789 = ?&#34;
LLM: 틀릴 수 있음
</code></pre><p>→ 복잡한 계산은 코드/도구 사용</p>
<h3 id="일관성-문제">일관성 문제<a hidden class="anchor" aria-hidden="true" href="#일관성-문제">#</a></h3>
<ul>
<li>같은 질문에 다른 답변</li>
<li>긴 대화에서 앞뒤 모순</li>
</ul>
<h3 id="편향">편향<a hidden class="anchor" aria-hidden="true" href="#편향">#</a></h3>
<ul>
<li>학습 데이터의 편향 반영</li>
<li>문화/성별/인종 편견 가능</li>
</ul>
<h3 id="비용">비용<a hidden class="anchor" aria-hidden="true" href="#비용">#</a></h3>
<ul>
<li>API 호출마다 과금</li>
<li>대량 사용 시 비용 부담</li>
</ul>
<hr>
<h2 id="9-핵심-요약">9. 핵심 요약<a hidden class="anchor" aria-hidden="true" href="#9-핵심-요약">#</a></h2>
<h3 id="llm이란">LLM이란?<a hidden class="anchor" aria-hidden="true" href="#llm이란">#</a></h3>
<ul>
<li>엄청난 텍스트로 학습한, 언어를 이해하고 생성하는 대규모 AI 모델</li>
<li>딥러닝 &gt; Transformer 기반</li>
</ul>
<h3 id="핵심-특징">핵심 특징<a hidden class="anchor" aria-hidden="true" href="#핵심-특징">#</a></h3>
<ul>
<li>Large: 수백억~조 개 파라미터</li>
<li>Language: 텍스트만 처리</li>
<li>Model: 학습된 Parameter</li>
</ul>
<h3 id="작동-원리">작동 원리<a hidden class="anchor" aria-hidden="true" href="#작동-원리">#</a></h3>
<ul>
<li>다음 토큰 확률 예측이 핵심</li>
<li>Transformer 구조 (Attention 메커니즘)</li>
<li>토큰 단위 처리</li>
</ul>
<h3 id="학습-과정-1">학습 과정<a hidden class="anchor" aria-hidden="true" href="#학습-과정-1">#</a></h3>
<ul>
<li>Pre-training: 다음 토큰 예측 학습</li>
<li>Fine-tuning: SFT + RLHF로 대화형 발전</li>
</ul>
<h3 id="잘하는-것">잘하는 것<a hidden class="anchor" aria-hidden="true" href="#잘하는-것">#</a></h3>
<ul>
<li>대화, 요약, 번역, 추론, 창작, 코딩</li>
</ul>
<h3 id="한계">한계<a hidden class="anchor" aria-hidden="true" href="#한계">#</a></h3>
<ul>
<li>할루시네이션, 지식 컷오프, 계산 약함, 비용</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://dont.kr/tags/ai/">Ai</a></li>
      <li><a href="https://dont.kr/tags/llm/">Llm</a></li>
      <li><a href="https://dont.kr/tags/transformer/">Transformer</a></li>
    </ul>
  </footer><script src="https://giscus.app/client.js"
    data-repo="duddns/dontkr-www"
    data-repo-id="R_kgDOJ5uOLg"
    data-category="General"
    data-category-id="DIC_kwDOJ5uOLs4CzVRe"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="preferred_color_scheme"
    data-lang="ko"
    crossorigin="anonymous"
    async>
</script>

</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://dont.kr/">DoNT - Do Not Think!!!</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
